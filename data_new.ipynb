{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "import time\n",
    "import torch, gc\n",
    "import glob\n",
    "import transformers\n",
    "import tokenizers\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageFile\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from pathlib import Path\n",
    "from datasets.utils.logging import set_verbosity_info\n",
    "from transformers import logging as tf_logging\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoProcessor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    shortcaption_image_folder: Optional[str] = field(default=None)\n",
    "    data_type: Optional[str] = field(default=\"mix\")\n",
    "    image_aspect_ratio: str = \"square\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    try:\n",
    "        metadata = sample[\"json\"]\n",
    "        return {\n",
    "            \"caption\": metadata.get(\"caption\"),\n",
    "            \"cot\": metadata.get(\"cot\"),\n",
    "            \"aspect_ratio\": metadata.get(\"aspect_ratio\"),\n",
    "            \"img_index\": metadata.get(\"img_index\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazySupervisedMixDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, #\"/home/v-haodongli/Janus/tmp_script/laion_2b_aesthetic/{00042..00133}.tar\"\n",
    "        processor: AutoProcessor,\n",
    "        # tokenizer: transformers.PreTrainedTokenizer,\n",
    "        # data_args: DataArguments,\n",
    "    ):\n",
    "        super(LazySupervisedMixDataset, self).__init__()\n",
    "\n",
    "        # self.data_args = data_args\n",
    "        list_data_dict = []\n",
    "        data_files = glob.glob(os.path.join(data_path, \"*.tar\"))\n",
    "        train_dataset = load_dataset(\"webdataset\", data_files=data_files, split=\"train\", num_proc=128)\n",
    "        train_dataset = train_dataset.map(process_sample).filter(lambda x: x is not None)\n",
    "        train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if not col in (\n",
    "            [\"caption\", \"cot\", \"aspect_ratio\", \"img_index\"])])\n",
    "        list_data_dict.append(train_dataset)\n",
    "        if len(list_data_dict) > 1:\n",
    "            list_data_dict = concatenate_datasets(list_data_dict)\n",
    "        else:\n",
    "            list_data_dict = list_data_dict[0]\n",
    "        list_data_dict = list_data_dict.shuffle(seed=42)\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.list_data_dict = list_data_dict\n",
    "        print(self.list_data_dict)\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            img_tokens = 128 if \"image\" in sample else 0\n",
    "            length_list.append(sum(len(conv[\"value\"].split()) for conv in sample[\"conversations\"]) + img_tokens)\n",
    "        return length_list\n",
    "\n",
    "    @property\n",
    "    def modality_lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            cur_len = sum(len(conv[\"value\"].split()) for conv in sample[\"conversations\"])\n",
    "            cur_len = cur_len if \"image\" in sample else -cur_len\n",
    "            length_list.append(cur_len)\n",
    "        return length_list\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "\n",
    "        conversation = [\n",
    "        {\"role\": \"<|User|>\", \"content\": sources['caption']},\n",
    "        {\"role\": \"<|Assistant|>\", \"content\": f\"{sources['cot']}<begin_of_image><end_of_image>\"},\n",
    "        ]\n",
    "        system_prompt = \"You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\"\n",
    "        prompt = self.processor.apply_sft_template_for_multi_turn_prompts(\n",
    "        conversations=conversation,\n",
    "        sft_format=self.processor.sft_format,\n",
    "        system_prompt=system_prompt,\n",
    "        )\n",
    "\n",
    "        # tokenize prompt\n",
    "        text_ids = self.processor.tokenizer.encode(prompt)\n",
    "        all_ids = text_ids[:-2] + sources['img_index'] + text_ids[-2:]\n",
    "        all_ids = torch.LongTensor(all_ids)\n",
    "\n",
    "        # 构建图像 token 的 mask\n",
    "        all_image_ids_mask = torch.zeros(all_ids.shape, dtype=torch.bool)\n",
    "        all_image_ids_mask[:] = False\n",
    "        all_image_ids_mask[-len(sources['img_index'])-2:-2] = True\n",
    "\n",
    "        # 找到 Assistant 回答开始的位置\n",
    "        try:\n",
    "            assistant_start_token_id = self.processor.tokenizer.encode(\"<|Assistant|>\")[0]\n",
    "            assistant_start_index = text_ids.index(assistant_start_token_id)\n",
    "        except (ValueError, IndexError):\n",
    "            assistant_start_index = 0\n",
    "\n",
    "        assistant_ids_mask = torch.zeros(all_ids.shape, dtype=torch.bool)\n",
    "        assistant_ids_mask[assistant_start_index:] = True\n",
    "\n",
    "        # 构造输入和标签\n",
    "        input_ids = all_ids[:-1]\n",
    "        text_ids_mask = all_image_ids_mask[:-1] == False\n",
    "        image_ids_mask = all_image_ids_mask[:-1]\n",
    "        label_ids = all_ids[1:]\n",
    "        label_text_ids_mask = assistant_ids_mask[1:] & (all_image_ids_mask[1:] == False)\n",
    "        label_image_ids_mask = assistant_ids_mask[1:] & all_image_ids_mask[1:]\n",
    "\n",
    "        data_dict = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"label_ids\": label_ids,\n",
    "            \"text_ids_mask\": text_ids_mask,\n",
    "            \"image_ids_mask\": image_ids_mask,\n",
    "            \"label_text_ids_mask\": label_text_ids_mask,\n",
    "            \"label_image_ids_mask\": label_image_ids_mask,\n",
    "        }\n",
    "        return data_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding ID: 100015\n",
      "Dataset({\n",
      "    features: ['caption', 'cot', 'aspect_ratio', 'img_index'],\n",
      "    num_rows: 202\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from janus.models.processing_vlm import VLChatProcessor\n",
    "processor: VLChatProcessor = VLChatProcessor.from_pretrained(\"deepseek-ai/Janus-Pro-7B\")\n",
    "tokenizer = processor.tokenizer\n",
    "padding_id = tokenizer.pad_token_id\n",
    "print(f\"Padding ID: {padding_id}\")\n",
    "train_dataset = LazySupervisedMixDataset(data_path=\"/home/v-haodongli/Janus/tmp_script/laion_2b_aesthetic\", processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([839])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids,_,_,_,_,_ = train_dataset[2]\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Dict\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset:\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        # 提取所有字段\n",
    "        input_ids, text_ids_mask, image_ids_mask, label_ids, label_text_ids_mask, label_image_ids_mask = (\n",
    "            [instance[key] for instance in instances]\n",
    "            for key in (\n",
    "                \"input_ids\",\n",
    "                \"text_ids_mask\",\n",
    "                \"image_ids_mask\",\n",
    "                \"label_ids\",\n",
    "                \"label_text_ids_mask\",\n",
    "                \"label_image_ids_mask\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 转换为张量\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        label_ids = pad_sequence(label_ids, batch_first=True, padding_value=-100)  # IGNORE_INDEX\n",
    "\n",
    "        # 对 mask 字段也进行 padding\n",
    "        text_ids_mask = pad_sequence(text_ids_mask, batch_first=True, padding_value=0)\n",
    "        image_ids_mask = pad_sequence(image_ids_mask, batch_first=True, padding_value=0)\n",
    "        label_text_ids_mask = pad_sequence(label_text_ids_mask, batch_first=True, padding_value=0)\n",
    "        label_image_ids_mask = pad_sequence(label_image_ids_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "        # 检查长度并截断\n",
    "        if input_ids.shape[1] > self.tokenizer.model_max_length:\n",
    "            print(f\"Warning: input length {input_ids.shape[1]} exceeds model max length {self.tokenizer.model_max_length}\")\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        label_ids = label_ids[:, :self.tokenizer.model_max_length]\n",
    "        text_ids_mask = text_ids_mask[:, :self.tokenizer.model_max_length]\n",
    "        image_ids_mask = image_ids_mask[:, :self.tokenizer.model_max_length]\n",
    "        label_text_ids_mask = label_text_ids_mask[:, :self.tokenizer.model_max_length]\n",
    "        label_image_ids_mask = label_image_ids_mask[:, :self.tokenizer.model_max_length]\n",
    "\n",
    "        # 构建最终 batch\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            label_ids=label_ids,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "            text_id_mask=text_ids_mask,\n",
    "            image_id_mask=image_ids_mask,\n",
    "            label_text_id_mask=label_text_ids_mask,\n",
    "            label_image_id_mask=label_image_ids_mask,\n",
    "        )\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[100000,   2054,    418,  ..., 100015, 100015, 100015],\n",
       "         [100000,   2054,    418,  ...,   1656,   3020, 100593]]),\n",
       " 'label_ids': tensor([[  2054,    418,    274,  ...,   -100,   -100,   -100],\n",
       "         [  2054,    418,    274,  ...,   3020, 100593, 100001]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]),\n",
       " 'text_id_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False,  True]]),\n",
       " 'image_id_mask': tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ...,  True,  True, False]]),\n",
       " 'label_text_id_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False,  True,  True]]),\n",
       " 'label_image_id_mask': tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ...,  True, False, False]])}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer) \n",
    "data_collator([train_dataset[1], train_dataset[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

wandb:
  entity: null
#  run_id: askkz9i2
  resume: 'auto'

experiment:
    project: "janus-SFT"
    name: "janus-SFT"
    output_dir: "janus-SFT"
    max_train_examples: 20000000
    max_train_examples_mmu: 40000000
    save_every: 10000
    eval_every: 2500
    generate_every: 1000
    log_every: 1
    log_grad_norm_every: 500
    resume_from_checkpoint: 'latest'

model:
    janus_pro:
        model_name_or_path: "deepseek-ai/Janus-Pro-7B"

    gradient_checkpointing: True

dataset:
    params:
        path: "/mnt/v-haodongli/cot_output_test_train"
        shuffle_buffer_size: 1000
        max_samples: 1000
        num_workers: 8
        resolution: 256
        pin_memory: True
        persistent_workers: True

    preprocessing:
        max_seq_length: 381 # for text tokens

optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 0.00002
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 1000

training:
    gradient_accumulation_steps: 1
    noise_type: "mask"
    batch_size: 4
    samples_per_epoch: 10000
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    max_train_steps: 10000
    overfit_one_batch: False
    cond_dropout_prob: 0.1
    min_masking_rate: 0.0
    label_smoothing: 0.0
    max_grad_norm: null
    guidance_scale: 0.0
    generation_timesteps: 12
    t2i_coeff: 1.0
    lm_coeff: 0.1
    mmu_coeff: 1.0
    cfg_weight: 5
    temperature: 1